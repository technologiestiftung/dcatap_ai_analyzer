{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze metadata semantically with an LLM 🤖\n",
    "\n",
    "This script retrieves a list of open government datasets from an online metadata API, filters them to keep only certain entries, and then uses an OpenAI language model to perform a semantic analysis of each dataset’s title and description. The analysis generates several scores (content, context, quality, and spatial), which are then combined with the original metadata. Finally, the script saves this enriched information to an Excel file, delivering an AI-driven summary of each dataset’s metadata. 🚀\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auto reload modules\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing necessary libraries and setting options...\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Display a message to inform the user about the import process\n",
    "print(\"Importing necessary libraries and setting options...\")\n",
    "\n",
    "pd.options.mode.chained_assignment = None\n",
    "pd.options.display.max_rows = 500\n",
    "pd.options.display.max_seq_items = 500\n",
    "\n",
    "from datetime import datetime\n",
    "from tqdm.notebook import tqdm\n",
    "import requests\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from utils import do_full_analysis, parse_analysis_results, parse_instructor_results\n",
    "import warnings\n",
    "import time\n",
    "import json\n",
    "import re\n",
    "\n",
    "pd.options.mode.chained_assignment = None\n",
    "pd.options.display.max_rows = 500\n",
    "pd.options.display.max_seq_items = 500\n",
    "\n",
    "warnings.simplefilter(\"ignore\", category=(UserWarning, FutureWarning))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API key is correctly set.\n"
     ]
    }
   ],
   "source": [
    "# Sanity check whether the API key is set\n",
    "\n",
    "import os\n",
    "import openai\n",
    "\n",
    "# Retrieve the API key from environment variables\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "if OPENAI_API_KEY is None:\n",
    "    raise ValueError(\"OPENAI_API_KEY is not set in the environment.\")\n",
    "else:\n",
    "    print(\"API key is correctly set.\")\n",
    "\n",
    "# Initialize the OpenAI client with the API key\n",
    "openai.api_key = OPENAI_API_KEY\n",
    "\n",
    "# Example usage\n",
    "OPENAI_SYSTEM_MESSAGE = \"You are a helpful assistant.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constants have been defined:\n",
      "BASELINK_DATASHOP: https://daten.berlin.de/datensaetze/\n",
      "MDV_DATA_PATH: _data/01_mdv_metadata.parq\n",
      "FIGSIZE: (7, 5)\n"
     ]
    }
   ],
   "source": [
    "# Constants\n",
    "# Dataset links are composed of this baselink and the identifier for each dataset.\n",
    "BASELINK_DATASHOP = \"https://daten.berlin.de/datensaetze/\"\n",
    "\n",
    "MDV_DATA_PATH = \"_data/01_mdv_metadata.parq\"\n",
    "\n",
    "# Default figure size for Quarto HTML output.\n",
    "FIGSIZE = (7, 5)\n",
    "\n",
    "# Print output to inform the user about the constants being set\n",
    "print(\"Constants have been defined:\")\n",
    "print(f\"BASELINK_DATASHOP: {BASELINK_DATASHOP}\")\n",
    "print(f\"MDV_DATA_PATH: {MDV_DATA_PATH}\")\n",
    "print(f\"FIGSIZE: {FIGSIZE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we perform a specific semantic analysis of titles and descriptions in our metadata catalog with LLMs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve data from the metadata API.\n",
    "\n",
    "The code snippet you provided retrieves metadata from an API and expects the JSON response to contain a key named `\"dataset\"`. This key should map to a list of dataset entries, which can be normalized into a DataFrame using `pd.json_normalize`.\n",
    "\n",
    "### Expected JSON Format\n",
    "\n",
    "The expected format of the JSON response from the API should look something like this:\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"dataset\": [\n",
    "    {\n",
    "      \"identifier\": \"dataset1\",\n",
    "      \"title\": \"Dataset Title 1\",\n",
    "      \"description\": \"Description of dataset 1\",\n",
    "      \"publisher\": {\n",
    "        \"name\": \"Publisher Name\"\n",
    "      },\n",
    "      \"keyword\": [\"keyword1\", \"keyword2\"]\n",
    "      // other fields...\n",
    "    },\n",
    "    {\n",
    "      \"identifier\": \"dataset2\",\n",
    "      \"title\": \"Dataset Title 2\",\n",
    "      \"description\": \"Description of dataset 2\",\n",
    "      \"publisher\": {\n",
    "        \"name\": \"Publisher Name\"\n",
    "      },\n",
    "      \"keyword\": [\"keyword3\", \"keyword4\"]\n",
    "      // other fields...\n",
    "    }\n",
    "    // more datasets...\n",
    "  ]\n",
    "}\n",
    "```\n",
    "\n",
    "### Key Points\n",
    "\n",
    "- The top-level key should be `\"dataset\"`, which contains an array of dataset objects.\n",
    "- Each dataset object should have fields like `\"identifier\"`, `\"title\"`, `\"description\"`, and `\"publisher\"`, among others.\n",
    "- The `pd.json_normalize` function will flatten nested structures, so if there are nested objects (like `\"publisher\"`), it will create separate columns for their fields.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"metadata.parquet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "MDV_API_LINK = (\n",
    "    \"https://datenregister.berlin.de/api/3/action/current_package_list_with_resources\"\n",
    ")\n",
    "\n",
    "\n",
    "def get_full_package_list(limit=500, sleep=2):\n",
    "    \"\"\"Get full package list from CKAN API\"\"\"\n",
    "    offset = 0\n",
    "    frames = []\n",
    "    while True:\n",
    "        print(f\"{offset} packages retrieved.\")\n",
    "        url = MDV_API_LINK + f\"?limit={limit}&offset={offset}\"\n",
    "        res = requests.get(url)\n",
    "        data = res.json()\n",
    "        if data[\"result\"] == []:\n",
    "            break\n",
    "        data = pd.DataFrame(pd.json_normalize(data[\"result\"]))\n",
    "        frames.append(data)\n",
    "        offset += limit\n",
    "        time.sleep(sleep)\n",
    "\n",
    "    data = pd.concat(frames)\n",
    "    data = data.reset_index(drop=True)\n",
    "\n",
    "    # Convert complex objects to strings for safe parquet storage\n",
    "    object_cols = data.select_dtypes(include=[\"object\"]).columns\n",
    "    for col in object_cols:\n",
    "        if data[col].notna().any() and isinstance(data[col].iloc[0], (list, dict)):\n",
    "            data[col] = data[col].apply(json.dumps)\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "# Retrieve metadata for all datasets\n",
    "df = get_full_package_list()\n",
    "\n",
    "# Save to parquet\n",
    "df.to_parquet(DATA_PATH)\n",
    "\n",
    "# Print the path and file that was saved\n",
    "print(f\"Saved the dataset to: {DATA_PATH}\")\n",
    "\n",
    "# Give user some info about the datasets\n",
    "print(\n",
    "    f\"We have {len(df):,.0f} datasets in the catalogue and {df.shape[1]} properties.\\n\"\n",
    ")\n",
    "display(df.info(memory_usage=\"deep\"))\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['author', 'author_email', 'berlin_source', 'berlin_type',\n",
       "       'creator_user_id', 'date_released', 'date_updated',\n",
       "       'geographical_coverage', 'geographical_granularity', 'id', 'isopen',\n",
       "       'license_id', 'license_title', 'license_url', 'maintainer',\n",
       "       'maintainer_email', 'metadata_created', 'metadata_modified', 'name',\n",
       "       'notes', 'num_resources', 'num_tags', 'owner_org', 'private', 'state',\n",
       "       'temporal_coverage_from', 'temporal_coverage_to',\n",
       "       'temporal_granularity', 'title', 'type', 'url', 'version', 'groups',\n",
       "       'resources', 'tags', 'relationships_as_subject',\n",
       "       'relationships_as_object', 'organization.id', 'organization.name',\n",
       "       'organization.title', 'organization.type', 'organization.description',\n",
       "       'organization.image_url', 'organization.created',\n",
       "       'organization.is_organization', 'organization.approval_status',\n",
       "       'organization.state', 'attribution_text', 'preview_image', 'extras',\n",
       "       'hvd_category', 'sample_record', 'username'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset from a parquet file into a DataFrame\n",
    "df = pd.read_parquet(DATA_PATH)\n",
    "\n",
    "# (Optional) Extract publisher information\n",
    "\n",
    "# In the Zürich dataset, the publisher was extracted by splitting the 'identifier' field.\n",
    "# However, Berlin’s CKAN metadata typically provides publisher information in a dedicated field,\n",
    "# such as 'organization'. If your Berlin data includes this field, you can extract the publisher like so:\n",
    "# if \"organization\" in df.columns:\n",
    "#     # If 'organization' is a dictionary (as is common with CKAN), extract its title:\n",
    "#     df[\"publisher\"] = df[\"organization\"].apply(\n",
    "#         lambda org: org.get(\"title\") if isinstance(org, dict) else org\n",
    "#     )\n",
    "# else:\n",
    "#     print(\"No 'organization' field found; skipping publisher extraction.\")\n",
    "\n",
    "# (Optional) Filtering step: In the Zürich code, datasets were filtered to retain only those tagged as \"ogd\".\n",
    "\n",
    "# In Berlin’s catalog (built on CKAN), all datasets are typically official Open Government Data.\n",
    "# If you still need to filter (e.g., by a specific tag like \"open-data\"), adjust the lambda accordingly.\n",
    "# if \"tags\" in df.columns:\n",
    "#     # Example: filtering for datasets that include the term \"open-data\" in their tags\n",
    "#     is_open_data = df.dropna(subset=[\"tags\"]).tags.apply(\n",
    "#         lambda x: \"open-data\" in x.lower()\n",
    "#     )\n",
    "#     df = df.loc[is_open_data[is_open_data].index]\n",
    "# else:\n",
    "#     print(\"No 'tags' column found; proceeding without tag-based filtering.\")\n",
    "\n",
    "# Reset the index of the DataFrame to ensure it is sequential after any filtering\n",
    "# df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Check if the filtered DataFrame is empty; if so, raise an error; otherwise, print dataset count\n",
    "# if len(df) == 0:\n",
    "#     raise ValueError(\"No data retrieved.\")\n",
    "# else:\n",
    "#     print(f\"Retrieved {len(df)} datasets.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_active_names(tag_entry):\n",
    "    tag_list = json.loads(tag_entry)\n",
    "    return [tag['name'] for tag in tag_list if tag['state'] == 'active']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['formatted_tags'] = df['tags'].apply(extract_active_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter down dataset\n",
    "df = df[(df['author'] != 'Stromnetz Berlin GmbH') & (~df['title'].str.contains('Sozialstatistisches Berichtswesen')) & (~df['title'].str.contains('Gesundheitsberichterstattung'))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze and score metadata\n",
    "\n",
    "#\n",
    "\n",
    "# > **⚠️ Warning:** Processing a high number of datasets using an LLM might cost a bit of time and money. An alternative would be to use a batch API or use a local LLM. Please confer the docs of your LLM provider. Just a heads up!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing to analyze all datasets datasets in parallel with 20 workers.\n",
      "Analysis has been completed for all chosen datasets.\n"
     ]
    }
   ],
   "source": [
    "# Be aware not to hit your OpenAI API rate limits with too many parallel requests.\n",
    "n_parallel = 20\n",
    "\n",
    "# Set a variable to specify the number of datasets to analyze. Default is None to process all datasets.\n",
    "num_datasets_to_analyze = (\n",
    "    None  # Change this value any number to None to process all datasets.\n",
    ")\n",
    "\n",
    "# Create a list of data rows to process in parallel, limited by the specified variable.\n",
    "data_rows = [x[1] for x in list(df.iterrows())]\n",
    "\n",
    "# If a number is specified, slice the data_rows; otherwise, use all datasets.\n",
    "if num_datasets_to_analyze is not None:\n",
    "    data_rows = data_rows[:num_datasets_to_analyze]\n",
    "\n",
    "dataset_count = (\n",
    "    num_datasets_to_analyze if num_datasets_to_analyze is not None else \"all datasets\"\n",
    ")\n",
    "print(\n",
    "    f\"Preparing to analyze {dataset_count} datasets in parallel with {n_parallel} workers.\"\n",
    ")\n",
    "\n",
    "# The analysis of the datasets will now begin using the specified number of parallel workers.\n",
    "with ThreadPoolExecutor(max_workers=n_parallel) as executor:\n",
    "    results = list(executor.map(do_full_analysis, data_rows))\n",
    "\n",
    "# Print the LLM output for each dataset to check for empty XML responses\n",
    "#for i, result in enumerate(results):\n",
    "    #print(f\"LLM output for dataset {i + 1}: {result}\")\n",
    "\n",
    "print(\"Analysis has been completed for all chosen datasets.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse instructor results\n",
    "results_parsed = parse_instructor_results(results)\n",
    "df_final = pd.concat([df.reset_index(), results_parsed], axis=1)\n",
    "cols = [\n",
    "    \"id\",\n",
    "    \"organization.title\",\n",
    "    \"title\",\n",
    "    \"notes\",\n",
    "    \"formatted_tags\",\n",
    "    'geographical_coverage',\n",
    "    'geographical_granularity',\n",
    "    'author',\n",
    "    'dateninhalt',\n",
    "    'dateninhalt_score',\n",
    "    'methodik',\n",
    "    'methodik_score',\n",
    "    'datenqualitaet',\n",
    "    'datenqualitaet_score',\n",
    "    'geographie',\n",
    "    'geographie_score',\n",
    "    'tag_qualitaet',\n",
    "    'tag_qualitaet_score',\n",
    "    'referenz',\n",
    "    'referenz_score'\n",
    "]\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d\")\n",
    "df_final[cols].to_excel(f\"_results/metadata_analysis_{timestamp}.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create a DataFrame from the results of the analysis, specifying the column name \"results_raw\".\n",
    "# This DataFrame will hold the raw results returned from the LLM analysis for each dataset.\n",
    "results_parsed = pd.DataFrame(results, columns=[\"results_raw\"])\n",
    "\n",
    "# Apply the function 'parse_analysis_results' to each entry in the \"results_raw\" column.\n",
    "# This function is expected to process the raw results and extract meaningful information.\n",
    "# Be cautious as this step may involve additional LLM calls, which can incur costs.\n",
    "results_parsed = results_parsed[\"results_raw\"].apply(parse_analysis_results)\n",
    "\n",
    "# Concatenate the list of DataFrames returned by 'parse_analysis_results' into a single DataFrame.\n",
    "# This is necessary because 'parse_analysis_results' may return a DataFrame for each row,\n",
    "# and we need to combine them into one cohesive DataFrame.\n",
    "results_parsed = pd.concat(results_parsed.tolist(), axis=0)\n",
    "\n",
    "# Reset the index of the concatenated DataFrame to ensure it is sequential after concatenation.\n",
    "# This helps maintain a clean DataFrame structure for further processing.\n",
    "results_parsed.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Combine the original DataFrame 'df' with the parsed results DataFrame 'results_parsed'.\n",
    "# This creates a final DataFrame that includes both the original dataset information and the analysis results.\n",
    "df_final = pd.concat([df, results_parsed], axis=1)\n",
    "\n",
    "# Define a list of columns that we want to retain in the final output.\n",
    "# These columns include identifiers and various scores that were generated during the analysis.\n",
    "cols = [\n",
    "    \"id\",\n",
    "    \"organization.title\",\n",
    "    \"title\",\n",
    "    \"notes\",\n",
    "    \"formatted_tags\",\n",
    "    'geographical_coverage',\n",
    "    'geographical_granularity',\n",
    "    'author',\n",
    "    'dateninhalt',\n",
    "    'dateninhalt_score',\n",
    "    'methodik',\n",
    "    'methodik_score',\n",
    "    'datenqualitaet',\n",
    "    'datenqualitaet_score',\n",
    "    'geographie',\n",
    "    'geographie_score',\n",
    "    'tag_qualitaet',\n",
    "    'tag_qualitaet_score',\n",
    "    'referenz',\n",
    "    'referenz_score'\n",
    "]\n",
    "\n",
    "# Generate a timestamp in the format YYYYMMDD to use in the filename for the output Excel file.\n",
    "# This helps in organizing results by date.\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d\")\n",
    "df_final[cols].to_excel(f\"_results/metadata_analysis_{timestamp}.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
